<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>A Unconscious Approach</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>




<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<div class="container-fluid main-container">

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->



<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Data Science Final Project</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="EDA.html">Exploratory Data Analysis</a>
</li>
<li>
  <a href="Model_1.html">MLE Approach</a>
</li>
<li>
  <a href="AnUnconsciousApproach.html">CNN Approach</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">A Unconscious Approach</h1>

</div>


<p>The cool thing about the Conscious approach, which we used in building <a href="link%20to%20other%20tab">the MLE classifier</a> is that we get to learn lots about how people draw things. Like, you can have a whole pizza, or you can have just a slice. Or most bananas dutifully obey the law of gravity, but some mischievous ones don’t.</p>
<p>Now, switching gears, think about this: What if we take an Unconscious approach? Put all that domain knowledge we have gained aside (as if we had never even looked at those images) and rely on machine brute force? Can a machine learning algorithm that does not benefit from our human visual perception (and is pretty much a black box) do a decent job at classifying <a href="https://quickdraw.withgoogle.com/"><em>Quick! Draw</em></a> images? We tried one that is popular for classifying images: a <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">convolutional neural network</a>. We name it <em>OurCNN</em> (not the news channel).</p>
<div id="ourcnn-specs" class="section level1">
<h1><em>OurCNN</em> specs</h1>
<div id="input" class="section level2">
<h2>Input</h2>
<p>32-by-32 image with black/white pixels</p>
</div>
<div id="output" class="section level2">
<h2>Output</h2>
<p>a label out of 31 edibles:</p>
<p>apple, asparagus, banana, birthday cake, blackberry, blueberry, bread, broccoli, cake, carrot, cookie, donut, grapes, hamburger, hot dog, ice cream, lollipop, mushroom, onion, peanut, pear, peas, pineapple, pizza, popsicle, potato, sandwich, steak, strawberry, string bean, watermelon</p>
<p>note: This is one category more than the number of categories that went into the MLE classifier. Just an error in copying text :-)</p>
</div>
<div id="inside-the-box" class="section level2">
<h2>Inside the box</h2>
<div id="a.-architecture" class="section level3">
<h3>a. Architecture</h3>
<p>2 convolution layers, each with the same structure:</p>
<ul>
<li>32 filters of size 3-by-3, applied with padding</li>
<li>ReLU activation followed by batch normalization</li>
</ul>
<p>1 max-pooling layer</p>
<ul>
<li>filters of size 2-by-2</li>
<li>followed by 30% dropout</li>
</ul>
<p>1 dense layer</p>
<ul>
<li>256 neurons</li>
<li>ReLU activation followed by batch normalization</li>
<li>30% dropout</li>
</ul>
<p>final layer</p>
<ul>
<li>31 neurons (for probabilities of the 31 categories)</li>
<li>softmax activation</li>
</ul>
</div>
<div id="b.-optimization" class="section level3">
<h3>b. Optimization</h3>
<p>loss: categorical cross-entropy</p>
<p>metric: accuracy</p>
<p>stochastic gradient descent:</p>
<ul>
<li>learning rate 0.001</li>
<li>decay 1e-6</li>
<li>momentum 0.9</li>
<li>Nesterov accelerated</li>
</ul>
</div>
<div id="c.-training" class="section level3">
<h3>c. Training</h3>
<p>20 epochs</p>
<p>batch size 32</p>
<p>use 5000 images per type, split 4:1 into training and validation sets – a fraction of the 140-300K images available per type</p>
<p>pick the model at the epoch that combines <em>lowest validation loss</em> and <em>highest validation accuracy</em>, here epoch 12 – see the training history below</p>
<p><img src="docs/TrangPlots/model2_history.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
<div id="d.-data" class="section level3">
<h3>d. Data</h3>
<div id="data-reduction" class="section level4">
<h4>Data reduction</h4>
<p><em>Quick! Draw</em> images are black-and-white and come in 256-by-256 resolution. The data are stroke data, i.e., for each stroke of the pen, a series of points are stored such that if you connect them with straight segments, you recover the stroke.</p>
<p>The MLE classifier keeps the original resolution, but uses only the points that define the segments.</p>
<p><em>OurCNN</em> uses those points and points between them, but reduces resolution to 32-by-32.</p>
<p>Here are the three data versions for a watermelon, a sandwich, and a pear.</p>
<p><img src="TrangPlots/data_reduction_watermelon.png" width="70%" style="display: block; margin: auto;" /><img src="TrangPlots/data_reduction_sandwich.png" width="70%" style="display: block; margin: auto;" /><img src="TrangPlots/data_reduction_pear.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Hmm, it is now clear that we trained the MLE classifier using less data than we did <em>OurCNN</em>.</p>
<p>Other data that neither classifiers use: time, country of origin, and also stroke sequence (relevant in a fascinating analysis of <a href="https://qz.com/994486/the-way-you-draw-circles-says-a-lot-about-you/">how people draw circles</a>).</p>
</div>
<div id="data-subsetting" class="section level4">
<h4>Data subsetting</h4>
<p>the MLE classifier was trained on a random sample of images of each type, regardless of whether Google AI recognized what type it is.</p>
<p><em>OurCNN</em> was trained on a random sample of images recognized by Google AI.</p>
<p>Here are two examples of Google-recognized and Google-unrecognized images.</p>
<p><img src="TrangPlots/rec_unrec_broccoli.png" width="70%" style="display: block; margin: auto;" /><img src="TrangPlots/rec_unrec_blackberry.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
</div>
</div>
</div>
<div id="ourcnn-performance" class="section level1">
<h1><em>OurCNN</em> performance</h1>
<div id="relative-to-google-ai" class="section level2">
<h2>Relative to Google AI</h2>
<p>We are, of course, curious to see how <em>OurCNN</em> performs compared to Google AI on each of these edibles. We took a random sample of 1000 images of each edible (regardless of whether they are recognized by Google AI or not) and put them through <em>OurCNN</em>. Below is how <em>OurCNN</em> performs on these images, compared to Google AI recognition rates based on the whole <a href="https://www.kaggle.com/c/quickdraw-doodle-recognition/data">dataset</a> available to us.</p>
<p><img src="TrangPlots/cnn_vs_google.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Hah, <em>OurCNN</em> is outperformed on ALL the edibles.</p>
<p>It is amazing that on those that <em>OurCNN</em> really struggles to differentiate, Google AI has recognition rates of 8x-9x percent! And this is even more awesome (for Google!) when you remind yourself that Google AI has to differentiate among 340 different things. <em>OurCNN</em> has the much simpler job of differentiating only 31 of those things (sorry, we don’t care much about non-edibles).</p>
</div>
<div id="on-google-recognized-images" class="section level2">
<h2>On Google-recognized images</h2>
<p>As <em>OurCNN</em> was trained on images whose category, it is appropriate to first look at how it performs on a random test set of Google-recognized images (again, 1000 images per edible). This performance is shown in the classification probability matrix below, where the edibles are sorted by classification accuracy.</p>
<p><img src="TrangPlots/recognized_ordered.png" width="70%" style="display: block; margin: auto;" /></p>
<p><em>OurCNN</em> does better for some edibles and worse for others. Like we saw before when building the MLE classifier, cakes and birthday cakes are often confused. And from the perspective of <em>OurCNN</em>, some blackberries look like grapes.</p>
<p>Let’s now zoom into the images that are misclassified and rearrange them a bit, so that we see what each edible tends to be misclassified as.</p>
<p><img src="TrangPlots/wrong_recognized_reordered.png" width="70%" style="display: block; margin: auto;" /></p>
<p>It looks like most of the confusion of <em>OurCNN</em> happen in pairs of edibles. Clearly, cake and birthday cake are a pair. Cakes are generally not cake-like enough because they look more like birthdday cakes, and birthday cakes are generally not birthday-y enough because they look more like cakes.</p>
<p>Other pairs are popsicle-lollipop, mushroom-broccoli, strawberry-pineapple, potato-cookie, pear-apple, string bean-banana, etc.</p>
<p>Some edibles are involves in more than one pair. Grapes is in a primary pair with blackberry, and in a secondary pair with peas. String been is paired with both banana and asparagus. Apple is paired with both blackberry and pear.</p>
<p>There are some one-way relationships. Cookie is the object of classification for bad pizza but much less the opposite. Steak is an object of classification for donut, but not the other way around. Quite a few bad pears look like peanuts, but few bad peanuts look like pears.</p>
<p>Let’s look at several of the pairs.</p>
<p><img src="TrangPlots/eight_pairs.png" width="80%" style="display: block; margin: auto;" /></p>
<p>Interesting! Some of them, we can kind of see why. Some of them, not.</p>
</div>
<div id="on-google-unrecognized-images" class="section level2">
<h2>On Google-unrecognized images</h2>
<p>We now subject <em>OurCNN</em> to a random set of Google-unrecognized images.</p>
<p><img src="TrangPlots/unrecognized_ordered.png" width="70%" style="display: block; margin: auto;" /></p>
<p>At the highest, <em>OurCNN</em> correctly classifies 38% of the pizza images in this set. At the lowest, 6.8% of the pear images. Contrast this with the probability of 0.032 when tossing a 31-sided coin.</p>
<p>It is interesting that there seem to be some favorites among the labels (blackberry and peas) for classifying any random category. Do bad drawings tend to have certain types of strokes that are commonly present in blackberry and/or peas drawings? Let’s look at some of these non-blackberries that are classified as blackberries (below, left) next to some good (Google-recognized) blackberries that <em>OurCNN</em> also classified as blackberries (below, right).</p>
<p><img src="other_as_blackberry.png" style="width:49.0%" /> <img src="blackberry_as_blackberry.png" style="width:49.0%" /></p>
<p>Perhaps the one thing in common between these two sets of images is that there are a lot of busy pixels!</p>
<p>Wait! Isn’t it super cool that <em>OurCNN</em> includes Blackberry the hand-held device?</p>
<p>To be complete, let’s also look at the classification probabilities of Google-unrecognized images that are misclassified by <em>OurCNN</em>.</p>
<p><img src="TrangPlots/wrong_unrecognized_reordered.png" width="70%" style="display: block; margin: auto;" /></p>
<p>We do not see anything additional here that was not noticed before.</p>
<!-- the overall Google-recognized rate for an equal-weight mixture of these 31 edibles is 93.2%. -->
</div>
</div>
<div id="conclusions" class="section level1">
<h1>Conclusions</h1>
<p>This Unconscious approach works quite well.</p>
<p>While <em>OurCNN</em> is nothing compared to Google AI, it is limited by several resources that we guess do not affect Google AI: (1) we use only a fraction of the data to train the classifier (5K images/edible, compared to the 140-300K images/edible available); (2) we wanted to try more powerful architectures, but could not handle them computationally.</p>
<p><em>OurCNN</em> in its current version took several days to train on one of our laptop computer without GPU and has to deal with other tasks while running this. If and when we manage to use Hopkins’s cluster computing resources for the training, we would be able to see if we can get a more powerful version of <em>OurCNN</em> and see how it compares with Google AI.</p>
<p>Comparing these two early products – or prototypes, to be precise – from the Conscious and Unconscious approaches, we venture to say that the Unconscious approach is a contender. Perhaps humans and machines have different ways to see things that both are valuable.</p>
<p>But, let’s recognize that such comparison is unreasonable. We know well (from the data reduction plots above) that the MLE classifier is malnourished. With a lot more data (both data from each image and number of images), the MLE classifier might be a lot more powerful. We need to try that and come back to you.</p>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
